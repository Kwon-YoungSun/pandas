# day09

# 머신러닝 데이터분석

1. 머신러닝 개요
    1-3. 머신러닝 프로세스
        1) 컴퓨터 알고리즘이 이해할 수 있는 형태로 데이터를 변환한다.
            ===> 따라서 판다스를 이용하여 데이터프레임으로 정리하는 과정이 필요하다.

        2) 여러 속성(변수) 간의 관계를 분석하여 결과를 예측하는 모형을 학습을 통해 찾는다.
            ===> 이 때 모형 학습에 사용하는 데이터를 훈련 데이터(train data)라고 부른다.

        3) 검증 과정을 통해 학습을 마친 모형의 예측 능력을 평가한다.
            ===> 학습을 마친 모형의 예측 능력을 평가하기 위한 데이터를 검증 데이터(test data)라고 말한다.

        4) 평가 결과를 바탕으로 최종 모형을 확정하여 문제 해결에 적용한다.

2. 회귀 분석: 지도학습
    가격, 매출, 주가, 환율, 수량 등 연속적인 값을 갖는 연속 변수를 예측하는 데 주로 활용된다.

    - 예측하고자 하는 목표를 종속(dependent) 변수 또는 예측(predictor) 변수라고 부른다.
    - 예측을 위해 모형이 사용하는 속성을 독립(independent) 변수 또는 설명(explanatory) 변수라고 한다.

    2-1. 단순회귀분석(Simple Linear Regression)     ===> ex01.py
        두 변수 사이에 일대일로 대응되는 확률적, 통계적 상관성을 찾는 알고리즘으로 대표적인 지도학습 유형

        수학적으로는 종속 변수 Y와 독립 변수 X 사이의 관계를 
            1차함수 Y=aX+b
        로 나타낸다.
        
        직선의 기울기(a)와 직선이 y축과 교차하는 지점인 y절편(b)을 반복 학습을 통해 찾는다.

    2-2. 다항회귀분석(Polynomial Regression)        ===> ex02.py
        2차함수 이상의 다항 함수를 이용하여 두 변수 간의 선형관계를 설명하는 알고리즘
        독립변수 X와 종속 변수 Y 사이에 선형의 상관관계가 있지만, 직선보다는 곡선으로 설명하는 것이 적합할 때 사용

        종속변수 Y와 독립변수 X 사이의 관계를
            Y = aX^2 + bX + c
        로 표시하여 설명한다.

    2-3. 다중회귀분석(Multivariate Regression)
        앞의 단순회귀분석, 다항회귀분석과 달리 여러 개의 독립 변수가 종속 변수에 영향을 주고 선형 관계를 갖는 경우에 사용

        종속변수 Y와 독립 변수 Xk (k 아래첨자) 간의 관계를
            Y = b + a1X1 + a2X2 + ... + anXn (숫자, n모두 아래첨자)
        와 같은 함수식으로 표현한다.

        각 독립 변수의 계수(a1, a2, a3, .... an)와 상수항(b)에 적절한 값들을 찾아서 모형을 완성

-----------------------------------------------------------------------------------------------------------------------------------

3. 분류: 지도학습
    예측하려는 대상의 속성(설명 변수)를 입력 받고, 목표 변수가 갖고 있는 카테고리(범주형) 값 중에서 어느 한 값으로 분류하여 예측

    고객 분류, 질병 진단, 스팸 메일 필터링, 음성 인식 등 목표 변수가 카테고리 값을 갖는 경우 사용
    KNN, SVM, Decision Tree, Logistic Regression 등

    3-1. KNN(K-Nearest-Neighbors)
        새로운 관측값이 주어지면 기존 데이터 중에서 가장 속성이 비슷한 k개의 이웃을 먼저 찾는다.
        그리고 가까운 이웃들이 갖고 있는 목표 값과 같은 값으로 분류하여 예측

        k 값에 따라서 예측 결과인 분류값이 달라지므로 적절한 k값을 찾는 것이 매우 중요하다.

    < 분류 모형의 예측력을 평가하는 지표 >
        1) Confusion Matrix
            모형이 예측하는 값에는 두 가지(True/False)가 있고, 각 예측값은 실제로 True이거나 False일 수 있다.
            모형의 예측값과 실제 값을 각각 축으로 하는 2 X 2 매트릭스로 표현한 것을 Confusion Matrix라고 부른다.

            예     --------------------------------------------
            측  T   TP(True Positive)  |  FP(False Positive)
            값     --------------------------------------------
                F   FN(False Negative) |  TN(True Negative)
                   --------------------------------------------
                            T                   F
                                    실제값
        2) 정확도(Precision)
            True로 예측한 분석대상 중에서 실제 값이 True인 비율
            모형의 정확성을 나타내는 지표가 된다.
            정확도가 높다는 것은 False Positive(실제 False를 True로 잘못 예측) 오류가 작다는 뜻
                Precision = TP / (TP + FP)

        3) 재현율(Recall)
            실제 값이 True인 분석대상 중에서 True로 예측하여 모형이 적중한 비율
            재현율이 높다는 것은 False Negative(실제 True를 False로 잘못 예측) 오류가 낮다는 뜻
                Recall = TP / (TP + FN)

        4) F1지표(F1-score)
            정확도와 재현율이 균등하게 반영될 수 있도록 정확도와 재현율의 조화평균을 계산한 것
            모형의 예측력을 종합적으로 평가하는 지표
            값이 높을수록 분류 모형의 예측력이 좋음

                F1 Score = 2 * (Precision * Recall) / (Precision + Recall) 

    3-2. SVM(Support Vector Machine)
        데이터셋의 여러 속성을 나타내는 데이터프레임의 각 열은 열 벡터 형태로 구현된다.
        열 벡터들이 각각 고유의 축을 갖는 벡터 공간을 만드는데, 분석 대상이 되는 개별 관측값은 모든 속성(열 벡터)에 관한 값을 해당 축의 좌표로 표시하여 벡터 공간에서의 위치를 나타낸다.

        속성(열 벡터)이 2개 존재하는 데이터셋은 2차원 평면 공간에 좌표로 표시, 속성이 3개이면 3차원 공간에 표시

        ===> SVM 모형은 벡터 공간에 위치한 훈련 데이터의 좌표와 각 데이터가 어떤 분류 값을 가져야 하는지 정답을 입력 받아서 학습함
             학습을 통해 벡터 공간을 나누는 경계를 찾으며, 같은 분류 값을 갖는 데이터끼리 같은 공간에 위치하도록 벡터 공간을 여러 조각으로 나눔

    3-3. 의사결정나무(Decision Tree)
        각 분기점마다 목표 값을 가장 잘 분류할 수 있는 속성을 찾아서 배치하고, 해당 속성이 갖는 값을 이용하여 새로운 가지(branch)를 만든다.

        각 분기점에서 최적의 속성을 선택할 때는 해당 속성을 기준으로 분류한 값들이 구분되는 정도를 측정한다.
        다른 종류의 값들이 섞여 있는 정도를 나타내는 Entropy가 낮을수록 분류가 잘 된 것이다.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
4. 군집(Clustering): 비지도학습
    - 데이터셋의 관측값이 갖고 있는 여러 속성을 분석하여 서로 비슷한 특징을 갖는 관측값끼리 같은 클러스터(집단)로 묶는 알고리즘
    - 어느 클러스터에도 속하지 못하는 관측값이 존재할 수 있는데, 이런 특성을 이용하여 특이 데이터(이상값, 중복값)을 찾는데 활용하기도 함
    - 분류 알고리즘과 달리 정답이 없는 상태에서 데이터 자체의 유사성만을 기준으로 판단

    - 신용카드 부정 사용 탐지, 구매 패턴 분석 등 소비자 행동 특성을 그룹화하는 데 사용됨

    4-1. K-Means
        - 데이터 간의 유사성을 측정하는 기준으로 각 클러스터의 중심까지의 거리 이용
        - 벡터 공간에 위치한 어떤 데이터에 대하여 k개의 클러스터가 주어졌을 때 클러스터의 중심까지의 거리가 가장 가까운 클러스터로 해당 데이터 할당
        - k값에 따라 모형의 성능이 달라짐. k가 클수록 정확도가 높아지지만, k값이 너무 커지면 선택지가 너무 많아지므로 분석 효과가 사라짐

    4-2. DBSCAN(Density-Based Spatial Clustering of Applications with Noise)
        - 데이터가 위치하고 있는 공간 밀집도를 기준으로 클러스터 구분
        - 자신을 중심으로 반지름 R의 공간에 최소 M개의 포인트가 존재하는 점을 코어포인트(core point)라고 부름
        - 코어 포인트는 아니지만 반지름 R 안에 다른 코어 포인트가 있을 경우 경계포인트(border point)라고 부름
        - 코어 포인트도 아니고 경계 포인트에도 속하지 않는 점을 Noise(또는 outlier)라고 분류한다.

        - 서로 밀접한 데이터끼리 하나의 클러스터를 구성하게 되고, 어느 클러스터에도 속하지 않는 점들은 Noise로 남는다.